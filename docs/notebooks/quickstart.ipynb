{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JAX Quickstart.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xtWX4x9DCF5_"
      },
      "source": [
        "# JAX Quickstart\n",
        "\n",
        "**JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.**\n",
        "\n",
        "With its updated version of [Autograd](https://github.com/hips/autograd), JAX\n",
        "can automatically differentiate native Python and NumPy code. It can\n",
        "differentiate through a large subset of Python’s features, including loops, ifs,\n",
        "recursion, and closures, and it can even take derivatives of derivatives of\n",
        "derivatives. It supports reverse-mode as well as forward-mode differentiation, and the two can be composed arbitrarily\n",
        "to any order.\n",
        "\n",
        "What’s new is that JAX uses\n",
        "[XLA](https://www.tensorflow.org/xla)\n",
        "to compile and run your NumPy code on accelerators, like GPUs and TPUs.\n",
        "Compilation happens under the hood by default, with library calls getting\n",
        "just-in-time compiled and executed. But JAX even lets you just-in-time compile\n",
        "your own Python functions into XLA-optimized kernels using a one-function API.\n",
        "Compilation and automatic differentiation can be composed arbitrarily, so you\n",
        "can express sophisticated algorithms and get maximal performance without having\n",
        "to leave Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SY8mDvEvCGqk",
        "colab": {}
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FQ89jHCYfhpg"
      },
      "source": [
        "## Multiplying Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xpy1dSgNqCP4"
      },
      "source": [
        "We'll be generating random data in the following examples. One big difference between NumPy and JAX is how you generate random numbers. For more details, see the [README](../../README.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u0nseKZNqOoH",
        "colab": {}
      },
      "source": [
        "key = random.PRNGKey(0)\n",
        "x = random.normal(key, (10,))\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hDJF0UPKnuqB"
      },
      "source": [
        "Let's dive right in and multiply two big matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eXn8GUl6CG5N",
        "colab": {}
      },
      "source": [
        "size = 3000\n",
        "x = random.normal(key, (size, size), dtype=np.float32)\n",
        "%timeit np.dot(x, x.T).block_until_ready()  # runs on the GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0AlN7EbonyaR"
      },
      "source": [
        "We added that `block_until_ready` because [JAX uses asynchronous execution by default](https://jax.readthedocs.io/en/latest/async_dispatch.html).\n",
        "\n",
        "JAX NumPy functions work on regular NumPy arrays. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZPl0MuwYrM7t",
        "colab": {}
      },
      "source": [
        "import numpy as onp  # original CPU-backed NumPy\n",
        "x = onp.random.normal(size=(size, size)).astype(onp.float32)\n",
        "%timeit np.dot(x, x.T).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_SrcB2IurUuE"
      },
      "source": [
        "That's slower because it has to transfer data to the GPU every time. You can ensure that an NDArray is backed by device memory using `device_put`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jj7M7zyRskF0",
        "colab": {}
      },
      "source": [
        "from jax import device_put\n",
        "\n",
        "x = onp.random.normal(size=(size, size)).astype(onp.float32)\n",
        "x = device_put(x)\n",
        "%timeit np.dot(x, x.T).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "clO9djnen8qi"
      },
      "source": [
        "The output of `device_put` still acts like an NDArray, but it only copies values back to the CPU when they're needed for printing, plotting, saving to disk, branching, etc. The behavior of `device_put` is equivalent to the function `jit(lambda x: x)`, but it's faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ghkfKNQttDpg"
      },
      "source": [
        "If you have a GPU (or TPU!) these calls run on the accelerator and have the potential to be much faster than on CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RzXK8GnIs7VV",
        "colab": {}
      },
      "source": [
        "x = onp.random.normal(size=(size, size)).astype(onp.float32)\n",
        "%timeit onp.dot(x, x.T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iOzp0P_GoJhb"
      },
      "source": [
        "JAX is much more than just a GPU-backed NumPy. It also comes with a few program transformations that are useful when writing numerical code. For now, there's three main ones:\n",
        "\n",
        " - `jit`, for speeding up your code\n",
        " - `grad`, for taking derivatives\n",
        " - `vmap`, for automatic vectorization or batching.\n",
        "\n",
        "Let's go over these, one-by-one. We'll also end up composing these in interesting ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bTTrTbWvgLUK"
      },
      "source": [
        "## Using `jit` to speed up functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YrqE32mvE3b7"
      },
      "source": [
        "JAX runs transparently on the GPU (or CPU, if you don't have one, and TPU coming soon!). However, in the above example, JAX is dispatching kernels to the GPU one operation at a time. If we have a sequence of operations, we can use the `@jit` decorator to compile multiple operations together using [XLA](https://www.tensorflow.org/xla). Let's try that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qLGdCtFKFLOR",
        "colab": {}
      },
      "source": [
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n",
        "\n",
        "x = random.normal(key, (1000000,))\n",
        "%timeit selu(x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a_V8SruVHrD_"
      },
      "source": [
        "We can speed it up with `@jit`, which will jit-compile the first time `selu` is called and will be cached thereafter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fh4w_3NpFYTp",
        "colab": {}
      },
      "source": [
        "selu_jit = jit(selu)\n",
        "%timeit selu_jit(x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HxpBc4WmfsEU"
      },
      "source": [
        "## Taking derivatives with `grad`\n",
        "\n",
        "In addition to evaluating numerical functions, we also want to transform them. One transformation is [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). In JAX, just like in [Autograd](https://github.com/HIPS/autograd), you can compute gradients with the `grad` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IMAgNJaMJwPD",
        "colab": {}
      },
      "source": [
        "def sum_logistic(x):\n",
        "  return np.sum(1.0 / (1.0 + np.exp(-x)))\n",
        "\n",
        "x_small = np.arange(3.)\n",
        "derivative_fn = grad(sum_logistic)\n",
        "print(derivative_fn(x_small))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PtNs881Ohioc"
      },
      "source": [
        "Let's verify with finite differences that our result is correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JXI7_OZuKZVO",
        "colab": {}
      },
      "source": [
        "def first_finite_differences(f, x):\n",
        "  eps = 1e-3\n",
        "  return np.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
        "                   for v in np.eye(len(x))])\n",
        "\n",
        "\n",
        "print(first_finite_differences(sum_logistic, x_small))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q2CUZjOWNZ-3"
      },
      "source": [
        "Taking derivatives is as easy as calling `grad`. `grad` and `jit` compose and can be mixed arbitrarily. In the above example we jitted `sum_logistic` and then took its derivative. We can go further:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TO4g8ny-OEi4",
        "colab": {}
      },
      "source": [
        "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yCJ5feKvhnBJ"
      },
      "source": [
        "For more advanced autodiff, you can use `jax.vjp` for reverse-mode vector-Jacobian products and `jax.jvp` for forward-mode Jacobian-vector products. The two can be composed arbitrarily with one another, and with other JAX transformations. Here's one way to compose them to make a function that efficiently computes full Hessian matrices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z-JxbiNyhxEW",
        "colab": {}
      },
      "source": [
        "from jax import jacfwd, jacrev\n",
        "def hessian(fun):\n",
        "  return jit(jacfwd(jacrev(fun)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TI4nPsGafxbL"
      },
      "source": [
        "## Auto-vectorization with `vmap`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PcxkONy5aius"
      },
      "source": [
        "JAX has one more transformation in its API that you might find useful: `vmap`, the vectorizing map. It has the familiar semantics of mapping a function along array axes, but instead of keeping the loop on the outside, it pushes the loop down into a function’s primitive operations for better performance. When composed with `jit`, it can be just as fast as adding the batch dimensions by hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TPiX4y-bWLFS"
      },
      "source": [
        "We're going to work with a simple example, and promote matrix-vector products into matrix-matrix products using `vmap`. Although this is easy to do by hand in this specific case, the same technique can apply to more complicated functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8w0Gpsn8WYYj",
        "colab": {}
      },
      "source": [
        "mat = random.normal(key, (150, 100))\n",
        "batched_x = random.normal(key, (10, 100))\n",
        "\n",
        "def apply_matrix(v):\n",
        "  return np.dot(mat, v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zWsc0RisQWx",
        "colab_type": "text"
      },
      "source": [
        "Given a function such as `apply_matrix`, we can loop over a batch dimension in Python, but usually the performance of doing so is poor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KWVc9BsZv0Ki",
        "colab": {}
      },
      "source": [
        "def naively_batched_apply_matrix(v_batched):\n",
        "  return np.stack([apply_matrix(v) for v in v_batched])\n",
        "\n",
        "print('Naively batched')\n",
        "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHfKaLE9stbA",
        "colab_type": "text"
      },
      "source": [
        "We know how to batch this operation manually. In this case, `np.dot` handles extra batch dimensions transparently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ipei6l8nvrzH",
        "colab": {}
      },
      "source": [
        "@jit\n",
        "def batched_apply_matrix(v_batched):\n",
        "  return np.dot(v_batched, mat.T)\n",
        "\n",
        "print('Manually batched')\n",
        "%timeit batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eF8Nhb-szAb",
        "colab_type": "text"
      },
      "source": [
        "However, suppose we had a more complicated function without batching support. We can use `vmap` to add batching support automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67Oeknf5vuCl",
        "colab": {}
      },
      "source": [
        "@jit\n",
        "def vmap_batched_apply_matrix(v_batched):\n",
        "  return vmap(apply_matrix)(v_batched)\n",
        "\n",
        "print('Auto-vectorized with vmap')\n",
        "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pYVl3Z2nbZhO"
      },
      "source": [
        "Of course, `vmap` can be arbitrarily composed with `jit`, `grad`, and any other JAX transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZfecIXaGFoj",
        "colab_type": "text"
      },
      "source": [
        "## Parallelization with `pmap`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PN9M-RyGJJF",
        "colab_type": "text"
      },
      "source": [
        "Suppose you want to run your computations on multiple XLA accelerators—GPUs or TPUs. JAX has an API for that called [pmap()](https://jax.readthedocs.io/en/latest/jax.html#jax.pmap) that lets you write single-program multiple-data (SPMD) programs. Google Cloud TPU pods are multi-host platforms and you can utilize them with this transform.\n",
        "\n",
        "Similar to `jit`, when you apply `pmap` to a function, you compile with XLA. Then, you execute it in parallel on multiple devices.\n",
        "\n",
        "Like `vmap`, this transform maps a function over array axes. But, unlike `vmap`, `pmap` replicates the function and executes each replica on its own XLA device in parallel. \n",
        "\n",
        "The transform also allows you to use all-reduce sum and other parallel SPMD collective operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFAQqLfVGL4H",
        "colab_type": "text"
      },
      "source": [
        "### Enable Cloud TPUs in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TttxraN5GMXF",
        "colab_type": "text"
      },
      "source": [
        "To take advantage of JAX's `pmap`, you'll be using Cloud TPUs in Google Colab:\n",
        "\n",
        "- Change your Google Colab runtime by clicking **Edit** > **Notebook settings**\n",
        "- Select **Hardware acceleration: TPU**\n",
        "- If it says \"Unable to connect to the runtime\", click on the **Reconnect** button in the top right corner and wait until it has a green check mark (✓)\n",
        "\n",
        "Then, run the cell below to further configure the TPU support:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI1x7ol_GUnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell inside Google Colab with TPUs enabled\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Import JAX libraries again, just in case (runtime switching resets the state)\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, pmap, random\n",
        "\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# This is required to use TPU Driver as JAX's backend\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoJbFTqtGl2k",
        "colab_type": "text"
      },
      "source": [
        "Your output should have the Colab TPU IP address (gRPC) and port number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f10Tw5PZGoKG",
        "colab_type": "text"
      },
      "source": [
        "- To list available devices for the backend, use [jax.devices()](https://jax.readthedocs.io/en/latest/jax.html#jax.devices):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnqzwBcgGp7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jax.devices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwhHa4LrGsmN",
        "colab_type": "text"
      },
      "source": [
        "(The default backend is 'gpu' or 'tpu', if available. Otherwise it's 'cpu'.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewx4DCImGxMf",
        "colab_type": "text"
      },
      "source": [
        "Let's go over some `pmap` examples.\n",
        "\n",
        "- For a number of XLA devices available (e.g. 8), you can use `pmap` to map along a leading array axis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCVRSahjG2mI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = pmap(lambda x: x ** 2)(jnp.arange(8))\n",
        "\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tpdh-ehG4he",
        "colab_type": "text"
      },
      "source": [
        "- Run a dot product multiply on several devices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yASda37nG6Jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a PRNG key and split it into 8 new ones\n",
        "keys = random.split(random.PRNGKey(0), 8)\n",
        "\n",
        "# Complile a new 5000x5000 matrix with `pmap`\n",
        "size = 5000\n",
        "matrices = pmap(lambda key: random.normal(key, (size, size)))(keys)\n",
        "\n",
        "# Run the dot product of two matrices in parallel on XLA accelerators\n",
        "result = pmap(jnp.dot)(matrices, matrices)\n",
        "\n",
        "# Return the mean of the result and show the output\n",
        "print(pmap(jnp.mean)(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2H12vPQG8HO",
        "colab_type": "text"
      },
      "source": [
        "- Measure the compute time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3yYESbEG8gL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timeit -n 5 -r 5 pmap(jnp.dot)(matrices, matrices).block_until_ready()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nOu6JrxHA2q",
        "colab_type": "text"
      },
      "source": [
        "- Compose autodiff (`grad`) with `pmap`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7NjrJabHDNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define an arbitrary array and reshape it\n",
        "x = jnp.arange(8.).reshape((4, 2))\n",
        "\n",
        "# Apply the @pmap decorator to run calculations across devices\n",
        "@pmap\n",
        "def f(x):\n",
        "  y = jnp.sin(x)\n",
        "  @pmap\n",
        "  def g(z):\n",
        "    return jnp.cos(z) * jnp.tan(y.sum()) * jnp.tanh(x).sum()\n",
        "  # Call `grad`!\n",
        "  return grad(lambda w: jnp.sum(g(w)))(x)\n",
        "\n",
        "f(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMIC2JUYHFn7",
        "colab_type": "text"
      },
      "source": [
        "The code here is simple. For a neural network example where you can do some data-parallel neural network training, check out [the SPMD MNIST example](https://github.com/google/jax/blob/master/examples/spmd_mnist_classifier_fromscratch.py) or the much more capable [Trax library](https://github.com/google/trax/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwNnjaI4th_8",
        "colab_type": "text"
      },
      "source": [
        "This is just a taste of what JAX can do. We're really excited to see what you do with it!"
      ]
    }
  ]
}